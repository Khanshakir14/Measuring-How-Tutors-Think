{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Track 1 mistral ai"
      ],
      "metadata": {
        "id": "TZ67hCrfjvBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RdSAEoGjuAS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import json, zipfile, random, re, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import xgboost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Load & flatten Track 1 dataset\n",
        "# -------------------------\n",
        "def load_flat(json_file, has_labels=True, label_key=\"Mistake_Identification\"):\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    rows = []\n",
        "    for item in data:\n",
        "        cid = item.get(\"conversation_id\")\n",
        "        ctx = item.get(\"conversation_history\", \"\")\n",
        "        tutors = item.get(\"tutor_responses\", {})\n",
        "        for tutor, info in tutors.items():\n",
        "            resp = info.get(\"response\", \"\")\n",
        "            label = None\n",
        "            if has_labels and isinstance(info.get(\"annotation\"), dict):\n",
        "                label = info[\"annotation\"].get(label_key)\n",
        "            rows.append({\n",
        "                \"conversation_id\": cid,\n",
        "                \"tutor\": tutor,\n",
        "                \"conversation_history\": ctx,\n",
        "                \"response\": resp,\n",
        "                \"text\": ctx + \"\\n\\nTUTOR: \" + resp,\n",
        "                \"label\": label\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------\n",
        "# Feature engineering\n",
        "# -------------------------\n",
        "sbert = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "\n",
        "def add_features(df):\n",
        "    sims = []\n",
        "    for i in range(0, len(df), 64):\n",
        "        batch = df.iloc[i:i+64]\n",
        "        emb_ctx = sbert.encode(batch['conversation_history'].astype(str).tolist(), convert_to_tensor=True)\n",
        "        emb_resp = sbert.encode(batch['response'].astype(str).tolist(), convert_to_tensor=True)\n",
        "        cos = util.pytorch_cos_sim(emb_ctx, emb_resp).diagonal().cpu().numpy()\n",
        "        sims.extend(cos.tolist())\n",
        "    df = df.copy().reset_index(drop=True)\n",
        "    df['sim'] = sims\n",
        "    df['has_error_terms'] = df['response'].str.contains(r'\\b(mistake|error|wrong|incorrect|issue|problem)\\b', case=False, na=False).astype(int)\n",
        "    df['has_fix_terms'] = df['response'].str.contains(r'\\b(correct|should|instead|try|fix|check)\\b', case=False, na=False).astype(int)\n",
        "    df['has_question'] = df['response'].str.contains(r'\\?', na=False).astype(int)\n",
        "    df['len_resp'] = df['response'].str.len().fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Oversampling\n",
        "# -------------------------\n",
        "def oversample_minority(df, label_col='label', target_per_class=1000):\n",
        "    counts = df[label_col].value_counts().to_dict()\n",
        "    parts = []\n",
        "    for lbl, grp in df.groupby(label_col):\n",
        "        if len(grp) >= target_per_class:\n",
        "            parts.append(grp.sample(n=target_per_class, random_state=SEED))\n",
        "        else:\n",
        "            need = target_per_class - len(grp)\n",
        "            sampled = grp.sample(n=need, replace=True, random_state=SEED)\n",
        "            sampled['response'] = sampled['response'].apply(lambda x: x.replace(\"Let's\", \"Let us\"))\n",
        "            sampled['text'] = sampled['conversation_history'] + \"\\n\\nTUTOR: \" + sampled['response']\n",
        "            parts.append(pd.concat([grp, sampled]))\n",
        "    return pd.concat(parts).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# Classical ensemble (TF-IDF + numeric)\n",
        "# -------------------------\n",
        "def train_feature_ensemble(train_df, feat_cols, label_col='label'):\n",
        "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=3)\n",
        "    X_text = tfidf.fit_transform(train_df['text'])\n",
        "    X_num = train_df[feat_cols].fillna(0).to_numpy()\n",
        "    X = np.hstack([X_text.toarray(), X_num])\n",
        "    y = train_df[label_col].astype(int).to_numpy()\n",
        "    rf = RandomForestClassifier(n_estimators=300, class_weight='balanced_subsample', random_state=SEED)\n",
        "    lr = LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')\n",
        "    xgb = xgboost.XGBClassifier(n_estimators=200, random_state=SEED, eval_metric='logloss', use_label_encoder=False)\n",
        "    voting = VotingClassifier(estimators=[('rf',rf),('lr',lr),('xgb',xgb)], voting='soft')\n",
        "    voting.fit(X, y)\n",
        "    cal = CalibratedClassifierCV(voting, method='isotonic', cv=3)\n",
        "    cal.fit(X, y)\n",
        "    return tfidf, cal\n",
        "\n",
        "# -------------------------\n",
        "# Transformer Trainer\n",
        "# -------------------------\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weight_tensor=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weight_tensor = class_weight_tensor.to(self.model.device) if class_weight_tensor is not None else None\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\").to(model.device)\n",
        "        outputs = model(**{k:v.to(model.device) for k,v in inputs.items() if k!=\"labels\"})\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weight_tensor) if self.class_weight_tensor is not None else nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"f1\": f1_score(labels, preds, average=\"macro\"), \"accuracy\": accuracy_score(labels, preds)}\n",
        "\n",
        "def prepare_hf_dataset(df, tokenizer, text_col='text', label_col='label', max_length=384):\n",
        "    df = df[df[label_col].notnull()].reset_index(drop=True)\n",
        "    enc = tokenizer(df[text_col].tolist(), truncation=True, padding='max_length', max_length=max_length)\n",
        "    enc['labels'] = df[label_col].astype(int).tolist()\n",
        "    return Dataset.from_dict(enc), df\n",
        "\n",
        "def train_transformer(train_df, val_df, model_name='microsoft/deberta-v3-base', num_labels=2, epochs=5, text_col='text'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    y = train_df['label'].astype(int).to_numpy()\n",
        "    cw = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "    class_weights = torch.tensor(cw, dtype=torch.float)\n",
        "    train_ds,_ = prepare_hf_dataset(train_df, tokenizer, text_col)\n",
        "    val_ds,_ = prepare_hf_dataset(val_df, tokenizer, text_col)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./tmp_mistake_model\",\n",
        "        eval_strategy=\"steps\", eval_steps=200, logging_steps=50,\n",
        "        learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
        "        num_train_epochs=epochs, weight_decay=0.01,\n",
        "        save_total_limit=1, load_best_model_at_end=True,save_steps=200,\n",
        "        metric_for_best_model=\"f1\", report_to=\"none\"\n",
        "    )\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weight_tensor=class_weights, model=model, args=args,\n",
        "        train_dataset=train_ds, eval_dataset=val_ds,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "    return tokenizer, trainer\n",
        "\n",
        "def predict_transformer_probs(tokenizer, trainer, df, text_col='text'):\n",
        "    enc = tokenizer(df[text_col].tolist(), truncation=True, padding='max_length', max_length=384)\n",
        "    ds = Dataset.from_dict(enc)\n",
        "    out = trainer.predict(ds)\n",
        "    return torch.softmax(torch.tensor(out.predictions), dim=1).numpy()\n",
        "\n",
        "# -------------------------\n",
        "# Label mapping\n",
        "# -------------------------\n",
        "LABEL_TO_INT = {'No':0, 'Yes':1, 'To some extent':2}\n",
        "INT_TO_LABEL = {v:k for k,v in LABEL_TO_INT.items()}\n",
        "def map_stageA_label(lbl):\n",
        "    if lbl=='No': return 0\n",
        "    if lbl in ('Yes','To some extent'): return 1\n",
        "    return None\n",
        "def map_stageB_label(lbl):\n",
        "    if lbl=='Yes': return 1\n",
        "    if lbl=='To some extent': return 0\n",
        "    return None\n",
        "\n",
        "# -------------------------\n",
        "# Blending & Threshold\n",
        "# -------------------------\n",
        "def tune_threshold(probs, y_true):\n",
        "    best=(0.5,0)\n",
        "    for t in np.linspace(0.1,0.9,17):\n",
        "        f1=f1_score(y_true,(probs[:,1]>t).astype(int),average='macro')\n",
        "        if f1>best[1]: best=(t,f1)\n",
        "    return best\n",
        "def find_best_blend(probs_t, probs_e, y_true):\n",
        "    best=(0,-1,0.5)\n",
        "    for w in np.linspace(0.3,1,8):\n",
        "        p=w*probs_t+(1-w)*probs_e\n",
        "        th,f1=tune_threshold(p,y_true)\n",
        "        if f1>best[1]: best=(w,f1,th)\n",
        "    return best\n",
        "\n",
        "# -------------------------\n",
        "# Hierarchical pipeline\n",
        "# -------------------------\n",
        "def train_hierarchical_pipeline(train_df_raw):\n",
        "    print(\"Adding features...\")\n",
        "    train_df=add_features(train_df_raw)\n",
        "    train_df['label']=train_df['label'].fillna('None')\n",
        "    print(\"Label counts:\", train_df['label'].value_counts().to_dict())\n",
        "\n",
        "    # --- Stage A\n",
        "    print(\"\\n--- Stage A: Mistake identified vs No ---\")\n",
        "    train_df['labelA']=train_df['label'].apply(map_stageA_label)\n",
        "    dfA=train_df[train_df['labelA'].notnull()].copy()\n",
        "    dfA['label']=dfA['labelA']\n",
        "    dfA_os=oversample_minority(dfA, label_col='label', target_per_class=1000)\n",
        "    feat_cols=['sim','has_error_terms','has_fix_terms','has_question','len_resp']\n",
        "    tfidfA,ensembleA=train_feature_ensemble(dfA_os,feat_cols)\n",
        "    trA,valA=train_test_split(dfA_os,test_size=0.15,stratify=dfA_os['label'],random_state=SEED)\n",
        "    tokA,trAmodel=train_transformer(trA,valA,num_labels=2)\n",
        "    valA_df=dfA.copy().reset_index(drop=True); valA_df['label_num']=valA_df['labelA']\n",
        "    p_tA=predict_transformer_probs(tokA,trAmodel,valA_df)\n",
        "    X_text=tfidfA.transform(valA_df['text']); X_num=valA_df[feat_cols].to_numpy()\n",
        "    p_eA=ensembleA.predict_proba(np.hstack([X_text.toarray(),X_num]))\n",
        "    wA,f1A,thA=find_best_blend(p_tA,p_eA,valA_df['label_num'])\n",
        "    print(f\"Stage A: w={wA:.2f}, thresh={thA:.2f}, F1={f1A:.4f}\")\n",
        "\n",
        "    # --- Stage B\n",
        "    print(\"\\n--- Stage B: Among identified -> Yes vs To some extent ---\")\n",
        "    sub=train_df[train_df['label'].isin(['Yes','To some extent'])].copy()\n",
        "    sub['labelB']=sub['label'].apply(map_stageB_label)\n",
        "    sub['label']=sub['labelB']\n",
        "    sub_os=oversample_minority(sub, label_col='label', target_per_class=800)\n",
        "    tfidfB,ensembleB=train_feature_ensemble(sub_os,feat_cols)\n",
        "    trB,valB=train_test_split(sub_os,test_size=0.15,stratify=sub_os['label'],random_state=SEED)\n",
        "    tokB,trBmodel=train_transformer(trB,valB,num_labels=2)\n",
        "    valB_df=sub.copy().reset_index(drop=True); valB_df['label_num']=valB_df['labelB']\n",
        "    p_tB=predict_transformer_probs(tokB,trBmodel,valB_df)\n",
        "    X_textB=tfidfB.transform(valB_df['text']); X_numB=valB_df[feat_cols].to_numpy()\n",
        "    p_eB=ensembleB.predict_proba(np.hstack([X_textB.toarray(),X_numB]))\n",
        "    wB,f1B,thB=find_best_blend(p_tB,p_eB,valB_df['label_num'])\n",
        "    print(f\"Stage B: w={wB:.2f}, thresh={thB:.2f}, F1={f1B:.4f}\")\n",
        "\n",
        "    return {'tfidfA':tfidfA,'ensA':ensembleA,'tokA':tokA,'modA':trAmodel,'wA':wA,'thA':thA,\n",
        "            'tfidfB':tfidfB,'ensB':ensembleB,'tokB':tokB,'modB':trBmodel,'wB':wB,'thB':thB,'feat_cols':feat_cols}\n",
        "\n",
        "# -------------------------\n",
        "# Inference\n",
        "# -------------------------\n",
        "def predict_hierarchical(art, df):\n",
        "    df=add_features(df)\n",
        "    feat=art['feat_cols']\n",
        "    # Stage A\n",
        "    p_tA=predict_transformer_probs(art['tokA'],art['modA'],df)\n",
        "    X_text=art['tfidfA'].transform(df['text']); X_num=df[feat].to_numpy()\n",
        "    p_eA=art['ensA'].predict_proba(np.hstack([X_text.toarray(),X_num]))\n",
        "    probsA=art['wA']*p_tA+(1-art['wA'])*p_eA\n",
        "    predsA=(probsA[:,1]>art['thA']).astype(int)\n",
        "    out=['']*len(df)\n",
        "    idx_yes=[i for i,p in enumerate(predsA) if p==1]\n",
        "    for i,p in enumerate(predsA):\n",
        "        if p==0: out[i]='No'\n",
        "    if idx_yes:\n",
        "        sub=df.iloc[idx_yes].reset_index(drop=True)\n",
        "        p_tB=predict_transformer_probs(art['tokB'],art['modB'],sub)\n",
        "        X_textB=art['tfidfB'].transform(sub['text']); X_numB=sub[feat].to_numpy()\n",
        "        p_eB=art['ensB'].predict_proba(np.hstack([X_textB.toarray(),X_numB]))\n",
        "        probsB=art['wB']*p_tB+(1-art['wB'])*p_eB\n",
        "        predsB=(probsB[:,1]>art['thB']).astype(int)\n",
        "        for j,idx in enumerate(idx_yes):\n",
        "            out[idx]='Yes' if predsB[j]==1 else 'To some extent'\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Build submission\n",
        "# -------------------------\n",
        "def build_submission(df, preds, out_zip=\"predictions_mistake_identification.json.zip\", label_key=\"Mistake_Identification\"):\n",
        "    grouped={}\n",
        "    for i,row in df.iterrows():\n",
        "        cid=row['conversation_id']\n",
        "        if cid not in grouped:\n",
        "            grouped[cid]={'conversation_id':cid,'conversation_history':row['conversation_history'],'tutor_responses':{}}\n",
        "        grouped[cid]['tutor_responses'][row['tutor']]={'response':row['response'],'annotation':{label_key:preds[i]}}\n",
        "    out=list(grouped.values())\n",
        "    with open(\"predictions_mistake_identification.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "        json.dump(out,f,ensure_ascii=False,indent=2)\n",
        "    with zipfile.ZipFile(out_zip,'w',compression=zipfile.ZIP_DEFLATED) as z:\n",
        "        z.write(\"predictions_mistake_identification.json\",arcname=\"predictions_mistake_identification.json\")\n",
        "    print(f\"Wrote {out_zip} with {len(out)} conversations.\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# MAIN\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    TRAIN=\"trainset.json\"; DEV=\"dev_testset.json\"; TEST=\"testset.json\"\n",
        "    tr=load_flat(TRAIN,has_labels=True)\n",
        "    dev=load_flat(DEV,has_labels=False)\n",
        "    test=load_flat(TEST,has_labels=False)\n",
        "    print(\"Shapes:\", tr.shape, dev.shape, test.shape)\n",
        "    arts=train_hierarchical_pipeline(tr)\n",
        "    print(\"Predicting dev/test...\")\n",
        "    dev_preds=predict_hierarchical(arts,dev)\n",
        "    test_preds=predict_hierarchical(arts,test)\n",
        "    print(pd.Series(test_preds).value_counts())\n",
        "    build_submission(dev,dev_preds,\"dev_mistake_identification.json.zip\")\n",
        "    build_submission(test,test_preds,\"test_mistake_identification.json.zip\")\n",
        "    print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "M0XH-slvj7pn",
        "outputId": "7e20260f-01dc-442a-96bf-e1c586d84b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (2476, 6) (333, 6) (1214, 6)\n",
            "Adding features...\n",
            "Label counts: {'Yes': 1932, 'No': 370, 'To some extent': 174}\n",
            "\n",
            "--- Stage A: Mistake identified vs No ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1065 21:01 < 01:22, 0.79 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.403600</td>\n",
              "      <td>0.331052</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.826667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.338500</td>\n",
              "      <td>0.417522</td>\n",
              "      <td>0.847145</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.263400</td>\n",
              "      <td>0.403678</td>\n",
              "      <td>0.868206</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.185100</td>\n",
              "      <td>0.602687</td>\n",
              "      <td>0.854203</td>\n",
              "      <td>0.856667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.141100</td>\n",
              "      <td>0.576121</td>\n",
              "      <td>0.851645</td>\n",
              "      <td>0.853333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage A: w=0.30, thresh=0.30, F1=0.9580\n",
            "\n",
            "--- Stage B: Among identified -> Yes vs To some extent ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [850/850 19:28, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.692800</td>\n",
              "      <td>0.666209</td>\n",
              "      <td>0.602732</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.614600</td>\n",
              "      <td>0.485913</td>\n",
              "      <td>0.787319</td>\n",
              "      <td>0.787500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.439500</td>\n",
              "      <td>0.540188</td>\n",
              "      <td>0.795904</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.304900</td>\n",
              "      <td>0.410204</td>\n",
              "      <td>0.870327</td>\n",
              "      <td>0.870833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage B: w=0.30, thresh=0.30, F1=0.9328\n",
            "Predicting dev/test...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes               1046\n",
            "No                 150\n",
            "To some extent      18\n",
            "Name: count, dtype: int64\n",
            "Wrote dev_mistake_identification.json.zip with 41 conversations.\n",
            "Wrote test_mistake_identification.json.zip with 150 conversations.\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRjAD00MkgsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}